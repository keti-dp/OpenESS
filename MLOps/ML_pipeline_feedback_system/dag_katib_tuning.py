"""
Katib í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ DAG (v1)
ë§¤ì›” 1ì¼ì— ì‹¤í–‰í•˜ì—¬ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•˜ê³  ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ GCSì— ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰ íë¦„:
1. Katib íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
2. ê° Trialì—ì„œ ëª¨ë¸ í•™ìŠµ ë° GCS ì €ì¥
3. ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ë² ìŠ¤íŠ¸ ëª¨ë¸ GCSì— ì €ì¥
4. ê²°ê³¼ í™•ì¸ ë° ì•Œë¦¼ ì „ì†¡

ì €ì¥ ìœ„ì¹˜:
- í•˜ì´í¼íŒŒë¼ë¯¸í„°: gs://keti-airflow-dataset/vt-model/{site_id}/models/{yyyymm}/{yyyymm}_xgboost_{site_id}.json
- ë² ìŠ¤íŠ¸ ëª¨ë¸: gs://keti-airflow-dataset/vt-model/{site_id}/models/{yyyymm}/{yyyymm}_xgboost_{site_id}_model.pkl
- ëª¨ë“  Trial ëª¨ë¸: gs://keti-airflow-dataset/vt-model/{site_id}/models/{yyyymm}/trials/
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
import pendulum
from pathlib import Path
import sys
import os
import json
import yaml

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

# ì‚¬ì´íŠ¸ ID ì§€ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
SITE_ID = os.getenv('SITE_ID', 'default_site')

# katib_config.yaml ë¡œë“œ
config_path = Path(__file__).parent / 'config' / 'katib_config.yaml'
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

if SITE_ID not in config['sites']:
    raise ValueError(f"ì‚¬ì´íŠ¸ '{SITE_ID}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸: {list(config['sites'].keys())}")

site_config = config['sites'][SITE_ID]
airflow_config = config['defaults']['airflow']
kubeflow_config = config['defaults']['kubeflow']


def run_katib_tuning(**context):
    """Katib í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    import tempfile
    from kfp import compiler
    from utils.kubeflow_client import KubeflowClient
    from pipeline.katib_pipeline import katib_tuning_pipeline

    ti = context['task_instance']
    execution_date = context['execution_date']

    print(f"[{site_config['name']}] Katib í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    print(f"ì‹¤í–‰ ë‚ ì§œ: {execution_date.strftime('%Y-%m-%d')}")

    # katib_config.yamlì˜ kubeflow ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° êµ¬ì„±
    early_stopping_config = kubeflow_config.get('early_stopping', {})

    katib_params = {
        'namespace': kubeflow_config.get('namespace', 'space-openess'),
        'experiment_name_prefix': SITE_ID,
        'timeout': kubeflow_config.get('katib_timeout', 7200),
        'max_trial_count': kubeflow_config.get('max_trial_count', 30),
        'parallel_trial_count': kubeflow_config.get('parallel_trial_count', 3),
        'training_image': kubeflow_config.get('training_image', 'ghcr.io/keti-dp/openess-public:keti.ai_maxvol_xgboost_models-0.6'),
        'parameters_config': json.dumps(kubeflow_config.get('katib_parameters', {})),
        # Early Stopping ì„¤ì •
        'early_stopping_enabled': early_stopping_config.get('enabled', True),
        'early_stopping_algorithm': early_stopping_config.get('algorithm', 'medianstop'),
        'early_stopping_min_trials': early_stopping_config.get('min_trials_required', 3),
        'early_stopping_start_step': early_stopping_config.get('start_step', 5)
    }

    print("\nKatib ì„¤ì • (katib_config.yamlì—ì„œ ë¡œë“œ):")
    print(f"  - Namespace: {katib_params['namespace']}")
    print(f"  - Experiment Prefix: {katib_params['experiment_name_prefix']}")
    print(f"  - Max trials: {katib_params['max_trial_count']}")
    print(f"  - Parallel trials: {katib_params['parallel_trial_count']}")
    print(f"  - Timeout: {katib_params['timeout']}s ({katib_params['timeout']//3600}h)")
    print(f"  - Early Stopping: {'Enabled' if katib_params['early_stopping_enabled'] else 'Disabled'}")
    if katib_params['early_stopping_enabled']:
        print(f"    - Algorithm: {katib_params['early_stopping_algorithm']}")
        print(f"    - Min trials: {katib_params['early_stopping_min_trials']}")
        print(f"    - Start step: {katib_params['early_stopping_start_step']}")
    print(f"  - ê²°ê³¼ ì €ì¥: PVC (/mnt/ess-dataset/{SITE_ID}/models/yyyymm/)")

    # Kubeflow í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    kf_client = KubeflowClient()

    # íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼
    with tempfile.TemporaryDirectory() as tmpdir:
        pipeline_path = os.path.join(tmpdir, 'katib_tuning_pipeline.yaml')

        print(f"\níŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì¤‘...")
        compiler.Compiler().compile(
            pipeline_func=katib_tuning_pipeline,
            package_path=pipeline_path
        )
        print(f"âœ“ íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì™„ë£Œ")

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        # Use simpler run_name to avoid MySQL collation issues
        import time
        run = kf_client.create_run(
            pipeline_path=pipeline_path,
            experiment_name=f'katib-tuning-{SITE_ID}',
            run_name=f"katib{int(time.time())}",  # Simpler name with timestamp
            params=katib_params
        )

    print(f"âœ“ íŒŒì´í”„ë¼ì¸ ì œì¶œ ì™„ë£Œ")
    print(f"  Run ID: {run.run_id}")

    # ì™„ë£Œ ëŒ€ê¸°
    print(f"\níŒŒì´í”„ë¼ì¸ ì™„ë£Œ ëŒ€ê¸° ì¤‘... (ìµœëŒ€ {katib_params['timeout']//3600}ì‹œê°„)")
    status = kf_client.wait_for_run_completion(run.run_id, timeout=katib_params['timeout'])

    # ê²°ê³¼ í™•ì¸
    success = status['status'] == 'SUCCEEDED' if status else False

    if not success:
        print(f"\nâš ï¸ Katib íŠœë‹ ì‹¤íŒ¨: {status['status'] if status else 'UNKNOWN'}")
        print("ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    else:
        print(f"\nâœ“ Katib íŠœë‹ ì™„ë£Œ!")
        print(f"  Run ID: {run.run_id}")
        print(f"  Status: {status['status']}")

    # ê²°ê³¼ë¥¼ XComì— ì €ì¥
    ti.xcom_push(key='katib_success', value=success)
    ti.xcom_push(key='run_id', value=run.run_id)
    ti.xcom_push(key='status', value=status['status'] if status else 'UNKNOWN')

    return {
        'success': success,
        'run_id': run.run_id,
        'status': status['status'] if status else 'UNKNOWN'
    }


def save_results(**context):
    """Katib ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥í•˜ê³  GCSì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸"""
    from google.cloud import storage

    ti = context['task_instance']
    execution_date = context['execution_date']

    katib_success = ti.xcom_pull(key='katib_success', task_ids='run_katib_tuning')
    run_id = ti.xcom_pull(key='run_id', task_ids='run_katib_tuning')

    print(f"\n{'='*60}")
    print(f"Katib íŠœë‹ ê²°ê³¼")
    print(f"{'='*60}")
    print(f"ì„±ê³µ ì—¬ë¶€: {'âœ“ ì„±ê³µ' if katib_success else 'âœ— ì‹¤íŒ¨'}")
    print(f"Run ID: {run_id}")
    print(f"ì‹¤í–‰ ë‚ ì§œ: {execution_date.strftime('%Y-%m-%d')}")

    # GCSì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
    best_params = None
    if katib_success:
        try:
            # GCS ê²½ë¡œ: gs://{bucket}/{model_base_path}/{site_id}/models/yyyymm/
            yearmonth = execution_date.strftime('%Y%m')
            filename = f"{yearmonth}_xgboost_{SITE_ID}.json"
            gcs_bucket = os.getenv('GCS_BUCKET', 'your-gcs-bucket')
            model_base_path = os.getenv('GCS_MODEL_BASE_PATH', 'vt-model')
            gcs_path = f"{model_base_path}/{SITE_ID}/models/{yearmonth}/{filename}"

            print(f"\nğŸ“¥ GCSì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ ì¤‘...")
            print(f"  ê²½ë¡œ: gs://{gcs_bucket}/{gcs_path}")

            storage_client = storage.Client()
            bucket = storage_client.bucket(gcs_bucket)
            blob = bucket.blob(gcs_path)

            if blob.exists():
                best_params = json.loads(blob.download_as_string())

                print(f"  âœ“ ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
                print(f"\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
                print(f"  {json.dumps(best_params['parameters'], indent=2)}")
                print(f"\nì„±ëŠ¥ ë©”íŠ¸ë¦­:")
                print(f"  {json.dumps(best_params['metrics'], indent=2)}")
            else:
                print(f"  âš ï¸ ìµœì  íŒŒë¼ë¯¸í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"  íŒŒì¼: gs://{gcs_bucket}/{gcs_path}")
        except Exception as e:
            print(f"  âš ï¸ ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    if katib_success and best_params:
        print(f"\nâœ“ Katib íŠœë‹ ì™„ë£Œ!")
        print(f"  - ë² ìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°: GCSì— ì €ì¥ë¨")
        print(f"  - ë² ìŠ¤íŠ¸ ëª¨ë¸: GCSì— ì €ì¥ë¨")
        kubeflow_ui = os.getenv('KUBEFLOW_UI', 'https://your-kubeflow-url')
        print(f"  - Kubeflow UI: {kubeflow_ui}")
    elif not katib_success:
        print(f"\nê¶Œì¥ ì‚¬í•­:")
        print(f"  - Katib ì‹¤í—˜ ë¡œê·¸ í™•ì¸")
        print(f"  - ë‹¤ì‹œ ì‹¤í–‰ ì‹œë„")

    print(f"{'='*60}\n")

    # ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
    result = {
        'site_id': SITE_ID,
        'site_name': site_config['name'],
        'execution_date': execution_date.strftime('%Y-%m-%d'),
        'run_id': run_id,
        'success': katib_success,
        'best_params': best_params['parameters'] if best_params else None,
        'metrics': best_params['metrics'] if best_params else None,
        'timestamp': datetime.now().isoformat()
    }

    output_dir = Path('/tmp/katib_results')
    output_dir.mkdir(exist_ok=True)

    output_file = output_dir / f"katib_{SITE_ID}_{execution_date.strftime('%Y%m')}.json"
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)

    print(f"âœ“ ê²°ê³¼ ì €ì¥: {output_file}")

    return result


def send_notification(**context):
    """ì™„ë£Œ ì•Œë¦¼ ì „ì†¡"""
    ti = context['task_instance']
    execution_date = context['execution_date']

    result = ti.xcom_pull(key='return_value', task_ids='save_results')

    message = f"""
{'='*60}
Katib í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ
{'='*60}

ì‚¬ì´íŠ¸: {result['site_name']} ({result['site_id']})
ì‹¤í–‰ ë‚ ì§œ: {result['execution_date']}
ìƒíƒœ: {'âœ“ ì„±ê³µ' if result['success'] else 'âœ— ì‹¤íŒ¨'}
Run ID: {result['run_id']}

Kubeflow UI: {os.getenv('KUBEFLOW_UI', 'https://your-kubeflow-url')}

{'='*60}
"""

    print(message)

    # ì—¬ê¸°ì— ì´ë©”ì¼, Slack ë“±ì˜ ì•Œë¦¼ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

    return message


# DAG ê¸°ë³¸ ì¸ì
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': airflow_config.get('retries', 1),
    'retry_delay': timedelta(minutes=airflow_config.get('retry_delay_minutes', 30)),
    'execution_timeout': timedelta(hours=3),  # Katibë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ 3ì‹œê°„ìœ¼ë¡œ ë‹¨ì¶•
}

# DAG ìƒì„±
dag = DAG(
    f'katib_tuning_{SITE_ID}',
    default_args=default_args,
    description=f'[{site_config["name"]}] Katib í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹',
    schedule=airflow_config.get('monthly_training_schedule', '0 3 1 * *'),  # ë§¤ì›” 1ì¼ ì˜¤ì „ 3ì‹œ
    start_date=pendulum.datetime(2025, 1, 1, tz='Asia/Seoul'),
    catchup=False,
    tags=['battery', 'katib', 'hyperparameter-tuning', SITE_ID],
    max_active_runs=airflow_config.get('max_active_runs', 1),
)

# Task ì •ì˜
katib_task = PythonOperator(
    task_id='run_katib_tuning',
    python_callable=run_katib_tuning,
    provide_context=True,
    dag=dag,
)

save_task = PythonOperator(
    task_id='save_results',
    python_callable=save_results,
    provide_context=True,
    dag=dag,
)

notification_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    provide_context=True,
    dag=dag,
)

# ëª¨ë¸ ê²€ì¦ ë° ë°°í¬ DAG íŠ¸ë¦¬ê±°
trigger_deployment = TriggerDagRunOperator(
    task_id='trigger_model_deployment',
    trigger_dag_id=f'model_validation_deployment_{SITE_ID}',
    wait_for_completion=False,  # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ (Deploy DAGê°€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰)
    dag=dag,
)

# Task ì˜ì¡´ì„±
katib_task >> save_task >> notification_task >> trigger_deployment
