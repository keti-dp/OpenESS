"""
ëª¨ë¸ ê²€ì¦ ë° ìë™ ë°°í¬ DAG
- ìƒˆ ëª¨ë¸ ê²€ì¦
- ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡± ì‹œ ìë™ ë°°í¬
- GCSì— ë°°í¬ ì´ë ¥ ê¸°ë¡
- KServe ìë™ ë¦¬ë¡œë“œ
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import pendulum
from pathlib import Path
import sys
import yaml

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

# ë°°í¬ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.deployment import (
    promote_model_to_production,
    record_deployment_to_gcs,
    trigger_kserve_reload
)

# ì‚¬ì´íŠ¸ ID ì§€ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
import os
SITE_ID = os.getenv('SITE_ID', 'default_site')

# deploy.yaml ë¡œë“œ
config_path = Path(__file__).parent / 'config' / 'deploy.yaml'
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

if SITE_ID not in config['sites']:
    raise ValueError(f"ì‚¬ì´íŠ¸ '{SITE_ID}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸: {list(config['sites'].keys())}")

site_config = config['sites'][SITE_ID]

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,  # ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'email': ['ml-team@your-company.com'],
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    f'model_validation_deployment_{SITE_ID}',
    default_args=default_args,
    description=f'[{SITE_ID}] ëª¨ë¸ ê²€ì¦ ë° ìë™ ë°°í¬ (GCS ì´ë ¥ ê´€ë¦¬ + KServe ìë™ ë¦¬ë¡œë“œ)',
    schedule=None,  # Katib DAGì—ì„œ íŠ¸ë¦¬ê±°
    start_date=pendulum.datetime(2025, 1, 1, tz='Asia/Seoul'),
    catchup=False,
    tags=[SITE_ID, 'model', 'validation', 'deployment', 'kserve'],  # SITE_IDë¥¼ ì²« ë²ˆì§¸ë¡œ
)




def load_models_from_gcs(**context):
    """GCSì—ì„œ ì‹ ê·œ/ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ"""
    from google.cloud import storage
    from google.oauth2 import service_account

    ti = context['task_instance']
    execution_date = context['execution_date']
    yearmonth = execution_date.strftime('%Y%m')

    # ì„¤ì •ì—ì„œ GCS ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    bucket_name = config['gcs']['bucket_name']
    credentials_path = config['gcs']['credentials_path']
    site_id = SITE_ID

    # ê²½ë¡œ ì„¤ì • (deploy.yaml ê¸°ë°˜)
    new_model_path = f"{site_config['paths']['models_dir']}/{yearmonth}/{yearmonth}_xgboost_{site_id}_model.pkl"
    current_model_path = f"{site_config['paths']['deploy_dir']}/{site_config['deploy_files']['model']}"

    # GCS ì¸ì¦
    credentials = service_account.Credentials.from_service_account_file(credentials_path)
    gcp_project = os.getenv('GCP_PROJECT', 'your-gcp-project')
    storage_client = storage.Client(credentials=credentials, project=gcp_project)
    bucket = storage_client.bucket(bucket_name)

    print(f"\n{'='*60}")
    print(f"ëª¨ë¸ ê²½ë¡œ í™•ì¸")
    print(f"{'='*60}")

    # 1. ìƒˆ ëª¨ë¸ ì¡´ì¬ í™•ì¸ (ë¡œë“œí•˜ì§€ ì•ŠìŒ)
    print(f"\nğŸ“¥ ìƒˆ ëª¨ë¸ ê²½ë¡œ í™•ì¸:")
    print(f"  ê²½ë¡œ: gs://{bucket_name}/{new_model_path}")

    new_model_blob = bucket.blob(new_model_path)
    if not new_model_blob.exists():
        raise FileNotFoundError(f"ìƒˆ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_model_path}")

    # ë©”íƒ€ë°ì´í„° ë¡œë“œ (size ì •ë³´ ê°€ì ¸ì˜¤ê¸°)
    new_model_blob.reload()
    print(f"  âœ“ ìƒˆ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ (í¬ê¸°: {new_model_blob.size / 1024:.2f} KB)")

    # 2. í˜„ì¬ ë°°í¬ëœ ëª¨ë¸ ì¡´ì¬ í™•ì¸ (ë¡œë“œí•˜ì§€ ì•ŠìŒ)
    current_model_blob = bucket.blob(current_model_path)
    current_model_exists = current_model_blob.exists()

    if current_model_exists:
        print(f"\nğŸ“¥ í˜„ì¬ ë°°í¬ ëª¨ë¸ ê²½ë¡œ í™•ì¸:")
        print(f"  ê²½ë¡œ: gs://{bucket_name}/{current_model_path}")
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        current_model_blob.reload()
        print(f"  âœ“ í˜„ì¬ ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ (í¬ê¸°: {current_model_blob.size / 1024:.2f} KB)")
    else:
        print(f"\nâš ï¸  í˜„ì¬ ë°°í¬ëœ ëª¨ë¸ ì—†ìŒ (ì²« ë°°í¬)")

    # XComìœ¼ë¡œ ì „ë‹¬ (ëª¨ë¸ ê²½ë¡œë§Œ ì „ë‹¬, ì‹¤ì œ ë¡œë“œëŠ” validate ë‹¨ê³„ì—ì„œ)
    ti.xcom_push(key='new_model_path', value=f'gs://{bucket_name}/{new_model_path}')
    ti.xcom_push(key='current_model_path', value=f'gs://{bucket_name}/{current_model_path}' if current_model_exists else None)
    ti.xcom_push(key='model_version', value=yearmonth)

    return {
        'new_model_exists': True,
        'current_model_exists': current_model_exists,
        'model_version': yearmonth
    }




def load_validation_data_from_gcs(**context):
    """GCSì—ì„œ ê²€ì¦ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (Parquet íŒŒì¼ë“¤)"""
    import tempfile
    from google.cloud import storage
    from google.oauth2 import service_account

    # shared_utilsëŠ” ì´ë¯¸ ìƒë‹¨ì—ì„œ sys.pathì— ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì§ì ‘ import
    from training.shared_utils import load_and_preprocess_validation_data

    ti = context['task_instance']
    site_id = SITE_ID

    # ì„¤ì •ì—ì„œ GCS ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    bucket_name = config['gcs']['bucket_name']
    credentials_path = config['gcs']['credentials_path']
    gcs_validation_prefix = 'val-dataset/'

    print(f"\n{'='*60}")
    print(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (GCS)")
    print(f"{'='*60}")
    print(f"  Bucket: gs://{bucket_name}")
    print(f"  ê²½ë¡œ: {gcs_validation_prefix}")

    try:
        # GCS í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        credentials = service_account.Credentials.from_service_account_file(credentials_path)
        gcp_project = os.getenv('GCP_PROJECT', 'your-gcp-project')
        storage_client = storage.Client(credentials=credentials, project=gcp_project)
        bucket = storage_client.bucket(bucket_name)

        # GCSì—ì„œ Parquet íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        print(f"\nğŸ“¥ GCSì—ì„œ Parquet íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        blobs = list(bucket.list_blobs(prefix=gcs_validation_prefix))
        parquet_blobs = [b for b in blobs if b.name.endswith('.parquet')]

        if not parquet_blobs:
            raise FileNotFoundError(
                f"GCSì— ê²€ì¦ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: gs://{bucket_name}/{gcs_validation_prefix}\n"
                f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:\n"
                f"  gsutil -m cp /path/to/*.parquet gs://{bucket_name}/{gcs_validation_prefix}"
            )

        print(f"  âœ“ {len(parquet_blobs)}ê°œ Parquet íŒŒì¼ ë°œê²¬")
        for blob in parquet_blobs[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"    - {blob.name}")
        if len(parquet_blobs) > 5:
            print(f"    ... ì™¸ {len(parquet_blobs) - 5}ê°œ")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        temp_dir = Path(tempfile.mkdtemp(prefix='validation_data_'))
        print(f"\nğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        print(f"  ë¡œì»¬ ê²½ë¡œ: {temp_dir}")

        downloaded_files = []
        for blob in parquet_blobs:
            local_path = temp_dir / Path(blob.name).name
            blob.download_to_filename(str(local_path))
            downloaded_files.append(local_path)
            print(f"  âœ“ {blob.name} â†’ {local_path.name}")

        print(f"\n  ì´ {len(downloaded_files)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # shared_utilsë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬
        print(f"\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        print(f"  í•¨ìˆ˜: load_and_preprocess_validation_data()")

        X_val, y_val, feature_cols = load_and_preprocess_validation_data(
            validation_path=str(temp_dir),
            target_col='RACK_MAX_CELL_VOLTAGE'
        )

        print(f"\n{'='*60}")
        print(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"  X_val shape: {X_val.shape}")
        print(f"  y_val shape: {y_val.shape}")
        print(f"  Features ({len(feature_cols)}): {feature_cols}")

        # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
        ti.xcom_push(key='validation_X', value=X_val.to_dict('list'))
        ti.xcom_push(key='validation_y', value=y_val.to_list())
        ti.xcom_push(key='validation_feature_cols', value=feature_cols)
        ti.xcom_push(key='validation_data_size', value=len(X_val))

        # ë¡œì»¬ íŒŒì¼ ì •ë¦¬
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        print(f"\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

        return {
            'validation_data_loaded': True,
            'data_size': len(X_val),
            'feature_count': len(feature_cols),
            'parquet_files_count': len(parquet_blobs)
        }

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise




def validate_new_model(**context):
    """ìƒˆ ëª¨ë¸ vs í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
    from google.cloud import storage
    import pickle
    import pandas as pd
    import io
    from sklearn.metrics import mean_squared_error, r2_score
    import numpy as np

    ti = context['task_instance']

    # XComì—ì„œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    new_model_path = ti.xcom_pull(key='new_model_path', task_ids='load_models')
    current_model_path = ti.xcom_pull(key='current_model_path', task_ids='load_models')

    # XComì—ì„œ ì „ì²˜ë¦¬ëœ ê²€ì¦ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    validation_X_dict = ti.xcom_pull(key='validation_X', task_ids='load_validation_data')
    validation_y_list = ti.xcom_pull(key='validation_y', task_ids='load_validation_data')
    feature_cols = ti.xcom_pull(key='validation_feature_cols', task_ids='load_validation_data')

    # ì„¤ì •ì€ ì´ë¯¸ ìƒë‹¨ì—ì„œ ë¡œë“œë¨
    validation_config = site_config['validation']

    # GCS ì¸ì¦
    from google.oauth2 import service_account
    bucket_name = config['gcs']['bucket_name']
    credentials_path = config['gcs']['credentials_path']
    credentials = service_account.Credentials.from_service_account_file(credentials_path)
    gcp_project = os.getenv('GCP_PROJECT', 'your-gcp-project')
    storage_client = storage.Client(credentials=credentials, project=gcp_project)
    bucket = storage_client.bucket(bucket_name)

    print(f"\n{'='*60}")
    print(f"ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦")
    print(f"{'='*60}")

    # 1. ê²€ì¦ ë°ì´í„° ë³µì›
    print(f"\nğŸ“¥ ê²€ì¦ ë°ì´í„° ë³µì›:")
    X_val = pd.DataFrame(validation_X_dict)
    y_val = pd.Series(validation_y_list)

    print(f"  - X_val shape: {X_val.shape}")
    print(f"  - y_val shape: {y_val.shape}")
    print(f"  - Features: {feature_cols}")

    # 2. ìƒˆ ëª¨ë¸ í‰ê°€
    new_model_blob = bucket.blob(new_model_path.replace(f'gs://{bucket_name}/', ''))
    new_model = pickle.loads(new_model_blob.download_as_bytes())

    y_pred_new = new_model.predict(X_val)

    new_rmse = np.sqrt(mean_squared_error(y_val, y_pred_new))
    new_r2 = r2_score(y_val, y_pred_new)

    print(f"\nğŸ“Š ìƒˆ ëª¨ë¸ ì„±ëŠ¥:")
    print(f"  - RMSE: {new_rmse:.4f}")
    print(f"  - RÂ²: {new_r2:.4f}")

    new_metrics = {
        'rmse': float(new_rmse),
        'r2': float(new_r2)
    }

    # 3. í˜„ì¬ ëª¨ë¸ê³¼ ë¹„êµ (ìˆëŠ” ê²½ìš°)
    deploy_decision = {'deploy': False}

    if current_model_path:
        current_model_blob = bucket.blob(current_model_path.replace(f'gs://{bucket_name}/', ''))
        current_model = pickle.loads(current_model_blob.download_as_bytes())

        y_pred_current = current_model.predict(X_val)

        current_rmse = np.sqrt(mean_squared_error(y_val, y_pred_current))
        current_r2 = r2_score(y_val, y_pred_current)

        print(f"\nğŸ“Š í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥:")
        print(f"  - RMSE: {current_rmse:.4f}")
        print(f"  - RÂ²: {current_r2:.4f}")

        # ê°œì„ ìœ¨ ê³„ì‚°
        rmse_improvement = (current_rmse - new_rmse) / current_rmse
        r2_improvement = (new_r2 - current_r2) / abs(current_r2) if current_r2 != 0 else 0

        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë³€í™”:")
        print(f"  - RMSE ê°œì„ : {rmse_improvement*100:.2f}%")
        print(f"  - RÂ² ê°œì„ : {r2_improvement*100:.2f}%")

        improvement = {
            'rmse_improvement_pct': float(rmse_improvement * 100),
            'r2_improvement_pct': float(r2_improvement * 100)
        }

        # ë°°í¬ ê¸°ì¤€ ê²€ì¦
        min_rmse_improvement = validation_config['min_improvement_rmse']
        max_r2_degradation = validation_config['max_degradation_r2']

        if rmse_improvement >= min_rmse_improvement:
            print(f"\nâœ… ë°°í¬ ìŠ¹ì¸: RMSE {rmse_improvement*100:.2f}% ê°œì„  (ê¸°ì¤€: {min_rmse_improvement*100}%)")
            deploy_decision = {
                'deploy': True,
                'reason': f'RMSE {rmse_improvement*100:.2f}% ê°œì„ ',
                'new_metrics': new_metrics,
                'current_metrics': {'rmse': float(current_rmse), 'r2': float(current_r2)},
                'improvement': improvement
            }
        elif r2_improvement < -max_r2_degradation:
            print(f"\nâŒ ë°°í¬ ê±°ë¶€: RÂ² {abs(r2_improvement)*100:.2f}% í•˜ë½ (ìµœëŒ€ í—ˆìš©: {max_r2_degradation*100}%)")
            deploy_decision = {
                'deploy': False,
                'reason': f'RÂ² ì„±ëŠ¥ í•˜ë½ ({abs(r2_improvement)*100:.2f}%)',
                'new_metrics': new_metrics,
                'current_metrics': {'rmse': float(current_rmse), 'r2': float(current_r2)},
                'improvement': improvement
            }
        else:
            print(f"\nâš ï¸  ë°°í¬ ë³´ë¥˜: ìµœì†Œ ê°œì„ ìœ¨ ë¯¸ë‹¬")
            deploy_decision = {
                'deploy': False,
                'reason': f'RMSE ê°œì„ ìœ¨ {rmse_improvement*100:.2f}% (ê¸°ì¤€: {min_rmse_improvement*100}%)',
                'new_metrics': new_metrics,
                'current_metrics': {'rmse': float(current_rmse), 'r2': float(current_r2)},
                'improvement': improvement
            }

    else:
        # ì²« ë°°í¬ì¸ ê²½ìš° ë¬´ì¡°ê±´ ìŠ¹ì¸
        print(f"\nâœ… ì²« ë°°í¬ - ìë™ ìŠ¹ì¸")
        deploy_decision = {
            'deploy': True,
            'reason': 'ì²« ë²ˆì§¸ ë°°í¬',
            'new_metrics': new_metrics,
            'improvement': {}
        }

    # XComìœ¼ë¡œ ì „ë‹¬
    ti.xcom_push(key='deploy_decision', value=deploy_decision)

    return deploy_decision




def decide_deployment_branch(**context):
    """ë°°í¬ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    ti = context['task_instance']
    deploy_decision = ti.xcom_pull(key='deploy_decision', task_ids='validate_model')

    if deploy_decision.get('deploy', False):
        return 'promote_to_production'
    else:
        return 'send_rejection_notification'




def send_deployment_notification(**context):
    """ë°°í¬ ì„±ê³µ ì•Œë¦¼"""
    ti = context['task_instance']
    metadata = ti.xcom_pull(key='deployment_metadata', task_ids='promote_to_production')
    deployment_id = ti.xcom_pull(key='deployment_id', task_ids='record_to_gcs')

    print(f"\n{'='*60}")
    print(f"âœ… ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"ì‚¬ì´íŠ¸: {metadata['site_id']}")
    print(f"ë²„ì „: {metadata['model_version']}")
    print(f"ë°°í¬ ID: {deployment_id}")
    print(f"ë°°í¬ ì‹œê°„: {metadata['deployed_at']}")
    print(f"ì„±ëŠ¥: {metadata['metrics']}")
    print(f"ê°œì„ ìœ¨: {metadata.get('improvement_over_previous', {})}")
    print(f"KServe ì—”ë“œí¬ì¸íŠ¸: {metadata['kserve_endpoint']}")
    print(f"\në°°í¬ ì´ë ¥: gs://{bucket_name}/{site_config['paths']['deploy_dir']}/deployment_history.json")

    # TODO: Slack/Teams ì•Œë¦¼ ì¶”ê°€
    # send_slack_notification(...)

    return True


def send_rejection_notification(**context):
    """ë°°í¬ ê±°ë¶€ ì•Œë¦¼"""
    ti = context['task_instance']
    deploy_decision = ti.xcom_pull(key='deploy_decision', task_ids='validate_model')

    print(f"\n{'='*60}")
    print(f"âŒ ëª¨ë¸ ë°°í¬ ê±°ë¶€")
    print(f"{'='*60}")
    print(f"ì‚¬ìœ : {deploy_decision.get('reason', 'Unknown')}")
    print(f"ìƒˆ ëª¨ë¸ ì„±ëŠ¥: {deploy_decision.get('new_metrics', {})}")
    print(f"í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥: {deploy_decision.get('current_metrics', {})}")
    print(f"ì„±ëŠ¥ ë³€í™”: {deploy_decision.get('improvement', {})}")

    # TODO: Slack/Teams ì•Œë¦¼ ì¶”ê°€

    return False




def verify_kserve_reload(**context):
    """KServeê°€ ìƒˆ ëª¨ë¸ì„ ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸ (SSHë¥¼ í†µí•´ Kubeflow ì„œë²„ì—ì„œ ì‹¤í–‰)"""
    import paramiko
    import time
    from pathlib import Path
    import os
    from dotenv import load_dotenv

    ti = context['task_instance']
    metadata = ti.xcom_pull(key='deployment_metadata', task_ids='promote_to_production')

    if not metadata:
        print("ë°°í¬ëœ ëª¨ë¸ì´ ì—†ìŒ - ê²€ì¦ ê±´ë„ˆëœ€")
        return True

    # SSH ì ‘ì† ì •ë³´ ë¡œë“œ
    config_dir = Path(__file__).parent / 'config'
    env_path = config_dir / '.env'
    load_dotenv(env_path)

    ssh_host = os.getenv('SSH_HOST')
    ssh_port = os.getenv('SSH_PORT')
    ssh_user = os.getenv('SSH_USER')
    ssh_password = os.getenv('SSH_PASSWORD')

    # KServe ì„¤ì •
    namespace = site_config['namespace']
    inference_service_name = site_config['kserve']['inference_service_name']

    print(f"\n{'='*60}")
    print(f"KServe ëª¨ë¸ ë¦¬ë¡œë“œ í™•ì¸ (SSH ì›ê²©)")
    print(f"{'='*60}")
    print(f"InferenceService: {inference_service_name}")
    print(f"Namespace: {namespace}")

    try:
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        print(f"\nğŸ”Œ SSH ì—°ê²° ì¤‘...")
        ssh_client.connect(
            hostname=ssh_host,
            port=int(ssh_port),
            username=ssh_user,
            password=ssh_password,
            timeout=30
        )
        print(f"  âœ“ SSH ì—°ê²° ì„±ê³µ")

        # kubectlë¡œ Pod ìƒíƒœ í™•ì¸
        check_cmd = (
            f"kubectl get pods -n {namespace} "
            f"-l serving.kserve.io/inferenceservice={inference_service_name} "
            f"-o jsonpath='{{.items[*].status.phase}}'"
        )

        print(f"\nğŸ“Š Pod ìƒíƒœ í™•ì¸ ì¤‘...")
        max_retries = 10
        for i in range(max_retries):
            sudo_cmd = f"echo '{ssh_password}' | sudo -S su -c \"{check_cmd}\""
            stdin, stdout, stderr = ssh_client.exec_command(sudo_cmd, timeout=30)
            exit_code = stdout.channel.recv_exit_status()

            if exit_code == 0:
                pod_status = stdout.read().decode('utf-8').strip()
                print(f"  Pod ìƒíƒœ: {pod_status}")

                if 'Running' in pod_status:
                    print(f"\nâœ“ KServe Podê°€ ì •ìƒ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")

                    # Pod ë¡œê·¸ì—ì„œ ëª¨ë¸ ë¡œë“œ í™•ì¸
                    log_cmd = (
                        f"kubectl logs -n {namespace} "
                        f"-l serving.kserve.io/inferenceservice={inference_service_name} "
                        f"--tail=20 | grep -i 'model\\|load\\|ready' || echo 'No logs found'"
                    )

                    sudo_log_cmd = f"echo '{ssh_password}' | sudo -S su -c \"{log_cmd}\""
                    stdin, stdout, stderr = ssh_client.exec_command(sudo_log_cmd, timeout=30)
                    stdout.channel.recv_exit_status()

                    logs = stdout.read().decode('utf-8')
                    print(f"\nğŸ“ ìµœê·¼ ë¡œê·¸:")
                    print(logs[:500] if logs else "  (ë¡œê·¸ ì—†ìŒ)")

                    ssh_client.close()
                    return True
                else:
                    print(f"  â³ Podê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ, 30ì´ˆ ëŒ€ê¸° ì¤‘... ({i+1}/{max_retries})")
                    if i < max_retries - 1:
                        time.sleep(30)
            else:
                error = stderr.read().decode('utf-8')
                print(f"  âŒ kubectl ëª…ë ¹ ì‹¤íŒ¨: {error}")
                break

        ssh_client.close()
        print(f"\nâš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, í•˜ì§€ë§Œ ë°°í¬ëŠ” ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼")
        return True

    except Exception as e:
        print(f"\nâŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

        # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ë°°í¬ ìì²´ëŠ” ì„±ê³µí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
        print(f"\nâš ï¸ ê²€ì¦ì€ ì‹¤íŒ¨í–ˆì§€ë§Œ ë°°í¬ëŠ” ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        return True




load_models = PythonOperator(
    task_id='load_models',
    python_callable=load_models_from_gcs,
    dag=dag,
)

load_validation_data = PythonOperator(
    task_id='load_validation_data',
    python_callable=load_validation_data_from_gcs,
    dag=dag,
)

validate_model = PythonOperator(
    task_id='validate_model',
    python_callable=validate_new_model,
    dag=dag,
)

decide_deployment = BranchPythonOperator(
    task_id='decide_deployment',
    python_callable=decide_deployment_branch,
    dag=dag,
)

promote_to_production = PythonOperator(
    task_id='promote_to_production',
    python_callable=promote_model_to_production,
    dag=dag,
)

record_to_gcs = PythonOperator(
    task_id='record_to_gcs',
    python_callable=record_deployment_to_gcs,
    dag=dag,
)

trigger_kserve = PythonOperator(
    task_id='trigger_kserve_reload',
    python_callable=trigger_kserve_reload,
    dag=dag,
)

verify_kserve = PythonOperator(
    task_id='verify_kserve_reload',
    python_callable=verify_kserve_reload,
    dag=dag,
)

send_success_notification = PythonOperator(
    task_id='send_success_notification',
    python_callable=send_deployment_notification,
    dag=dag,
)

# ê±°ë¶€ ë¸Œëœì¹˜
send_rejection_notification = PythonOperator(
    task_id='send_rejection_notification',
    python_callable=send_rejection_notification,
    dag=dag,
)

join = EmptyOperator(
    task_id='join',
    trigger_rule='none_failed_min_one_success',
    dag=dag,
)



load_models >> validate_model
load_validation_data >> validate_model

validate_model >> decide_deployment

# ë°°í¬ ê²½ë¡œ
decide_deployment >> promote_to_production >> record_to_gcs >> trigger_kserve >> verify_kserve >> send_success_notification >> join

# ê±°ë¶€ ê²½ë¡œ
decide_deployment >> send_rejection_notification >> join
